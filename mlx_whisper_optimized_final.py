#!/usr/bin/env python3
"""
Final optimized WhisperX-MLX implementation with accurate word timestamps.

This implementation achieves:
1. True single-stage processing (no redundant forward passes)
2. Cross-attention weight collection during batch decoding
3. DTW-based word alignment using collected attention weights
4. High throughput with accurate word-level timestamps
"""

import mlx.core as mx
import numpy as np
from typing import List, Dict, Tuple, Optional, Any
import time
import json
from dataclasses import dataclass
import warnings

from mlx_whisper.load_models import load_model
from mlx_whisper.audio import log_mel_spectrogram, pad_or_trim
from mlx_whisper.decoding import DecodingOptions, DecodingResult
from mlx_whisper.tokenizer import get_tokenizer
from mlx_whisper.timing import dtw
from median_filter_fix import median_filter_fixed as median_filter

# Import batch decoder components
import mlx_whisper_batch_decoder
from mlx_whisper_batch_decoder import BatchDecodingTask, BatchInference
from mlx_ultra_optimized_batch import install_broadcasting_fix


# Global storage for cross-attention weights
_cross_attention_data = {}


class CrossAttentionBatchInference(BatchInference):
    """
    Extended BatchInference that collects cross-attention weights during decoding.
    
    This is the key to achieving true single-stage processing with word timestamps.
    """
    
    def __init__(self, model: "Whisper", batch_size: int):
        super().__init__(model, batch_size)
        self.cross_attention_weights = [[] for _ in range(batch_size)]
        
    def logits(self, tokens: mx.array, audio_features: mx.array,
               sequence_mask: Optional[mx.array] = None) -> mx.array:
        """
        Compute logits for batch AND collect cross-attention weights.
        """
        global _cross_attention_data
        
        # If no mask provided, assume all sequences are active
        if sequence_mask is None:
            sequence_mask = mx.ones(self.batch_size, dtype=mx.bool_)
            
        # Get active indices
        mask_np = np.array(sequence_mask)
        active_indices = np.where(mask_np)[0]
        
        if len(active_indices) == 0:
            return mx.zeros((self.batch_size, self.model.dims.n_vocab))
        
        # Extract active sequences
        active_indices_mx = mx.array(active_indices)
        active_tokens = mx.take(tokens, active_indices_mx, axis=0)
        active_audio = mx.take(audio_features, active_indices_mx, axis=0)
        
        # Forward through decoder
        if self.kv_cache is None:
            # First iteration
            outputs = self.model.decoder(active_tokens, active_audio)
            logits = outputs[0][:, -1]  # Last token
            new_kv_cache = outputs[1]
            
            # Collect cross-attention if available
            if len(outputs) >= 3 and outputs[2] is not None:
                self._store_cross_attention(outputs[2], active_indices)
                
            self.kv_cache = self._expand_kv_cache(new_kv_cache, active_indices_mx)
        else:
            # Subsequent iterations
            active_kv_cache = self._extract_active_kv_cache(active_indices_mx)
            
            outputs = self.model.decoder(
                active_tokens, active_audio, kv_cache=active_kv_cache
            )
            logits = outputs[0][:, -1]
            new_kv_cache = outputs[1]
            
            # Collect cross-attention if available
            if len(outputs) >= 3 and outputs[2] is not None:
                self._store_cross_attention(outputs[2], active_indices)
                
            self._update_kv_cache(new_kv_cache, active_indices_mx)
            
        # Expand logits back to full batch
        full_logits = mx.zeros((self.batch_size, logits.shape[-1]))
        for i, idx in enumerate(active_indices):
            idx_int = int(idx)
            full_logits[idx_int] = logits[i]
            
        return full_logits.astype(mx.float32)
    
    def _store_cross_attention(self, cross_qk: List, active_indices: np.ndarray):
        """Store cross-attention weights for active sequences."""
        global _cross_attention_data
        
        for seq_idx, global_idx in enumerate(active_indices):
            layer_weights = []
            
            for layer_qk in cross_qk:
                if layer_qk is not None and seq_idx < layer_qk.shape[0]:
                    layer_weights.append(layer_qk[seq_idx])
                else:
                    layer_weights.append(None)
                    
            self.cross_attention_weights[global_idx].append(layer_weights)
            
            # Also store globally
            if global_idx not in _cross_attention_data:
                _cross_attention_data[global_idx] = []
            _cross_attention_data[global_idx].append(layer_weights)


def extract_words_with_dtw(
    tokens: List[int],
    cross_attention_weights: List,
    mel_shape: Tuple[int, int],
    model: Any,
    tokenizer: Any,
    debug: bool = False
) -> List[Dict[str, Any]]:
    """
    Extract word timestamps using DTW on cross-attention weights.
    
    This is where the magic happens - we use the attention patterns from
    the model to determine when each word was spoken.
    """
    # Filter text tokens (exclude special tokens)
    text_tokens = [t for t in tokens if t < tokenizer.eot]
    if not text_tokens or not cross_attention_weights:
        return []
    
    # Get alignment heads (specific attention heads used for alignment)
    alignment_heads = model.alignment_heads
    
    # Collect attention weights from alignment heads
    all_weights = []
    
    for step_idx, step_weights in enumerate(cross_attention_weights):
        if not step_weights or step_idx >= len(text_tokens):
            continue
            
        head_weights = []
        
        for layer_idx, head_idx in alignment_heads.tolist():
            if layer_idx < len(step_weights) and step_weights[layer_idx] is not None:
                layer_weight = step_weights[layer_idx]
                if head_idx < layer_weight.shape[0]:
                    # Get attention weights for the current token position
                    seq_len = layer_weight.shape[1]
                    if seq_len > 0:
                        # For each step, we look at the attention from the last generated token
                        weight = layer_weight[head_idx, -1, :]
                        head_weights.append(weight)
        
        if head_weights:
            avg_weight = mx.stack(head_weights).mean(axis=0)
            all_weights.append(avg_weight)
    
    if not all_weights:
        return []
    
    # Stack weights for all text tokens
    weights = mx.stack(all_weights[:len(text_tokens)])
    
    if debug:
        print(f"  Attention weights shape: {weights.shape}")
        print(f"  Text tokens: {len(text_tokens)}, Weight steps: {len(all_weights)}")
    
    # Apply temperature scaling and softmax for sharper peaks
    temperature = 10.0  # Higher temperature = sharper attention peaks
    weights = mx.softmax(weights * temperature, axis=-1)
    
    # Convert to numpy for processing
    weights_np = np.array(weights).astype(np.float32)
    
    # Apply median filter to smooth out noise
    weights_np = median_filter(weights_np, 7)
    
    # Normalize weights per token (zero mean, unit variance)
    mean = weights_np.mean(axis=1, keepdims=True)
    std = weights_np.std(axis=1, keepdims=True) + 1e-8
    weights_np = (weights_np - mean) / std
    
    # Run DTW alignment
    # Negative weights because DTW finds minimum cost path
    alignment = dtw(-weights_np.T)
    
    if debug:
        print(f"  DTW alignment shape: {alignment.shape}")
        if alignment.shape[1] > 0:
            print(f"  Alignment range: [{alignment[0, 0]}, {alignment[0, -1]}]")
    
    # Convert alignment to word timestamps
    words = []
    token_strs = [tokenizer.decode([t]) for t in text_tokens]
    
    current_word = ""
    word_start_idx = 0
    
    # Group tokens into words (tokens starting with space indicate new words)
    for i, token_str in enumerate(token_strs):
        if i > 0 and token_str.startswith(" "):
            # Save previous word
            if current_word.strip():
                # Get aligned frames
                start_frame = alignment[0, word_start_idx] if word_start_idx < alignment.shape[1] else 0
                end_frame = alignment[0, i-1] if i-1 < alignment.shape[1] else start_frame
                
                # Ensure end >= start
                end_frame = max(end_frame, start_frame)
                
                # Convert to seconds (50Hz = 0.02s per frame)
                words.append({
                    "word": current_word.strip(),
                    "start": float(start_frame * 0.02),
                    "end": float(end_frame * 0.02),
                    "probability": 1.0
                })
            
            current_word = token_str
            word_start_idx = i
        else:
            current_word += token_str
    
    # Don't forget the last word
    if current_word.strip() and word_start_idx < alignment.shape[1]:
        start_frame = alignment[0, word_start_idx]
        end_frame = alignment[0, -1] if alignment.shape[1] > 0 else start_frame
        end_frame = max(end_frame, start_frame)
        
        words.append({
            "word": current_word.strip(),
            "start": float(start_frame * 0.02),
            "end": float(end_frame * 0.02),
            "probability": 1.0
        })
    
    return words


class OptimizedWhisperProcessor:
    """
    Optimized WhisperX-MLX processor with true single-stage word timestamps.
    
    Key features:
    - Batch processing for high throughput
    - Cross-attention weight collection during decoding
    - DTW-based word alignment without redundant forward passes
    - Proper handling of long audio files through chunking
    """
    
    def __init__(self, model_name: str = "mlx-community/whisper-large-v3-mlx"):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.debug = False
        
    def load(self):
        """Load model and install fixes."""
        if self.model is None:
            print("Loading model...")
            self.model = load_model(self.model_name, dtype=mx.float16)
            self.tokenizer = get_tokenizer(
                self.model.is_multilingual,
                num_languages=getattr(self.model, 'num_languages', 99),
                language="en",
                task="transcribe",
            )
            install_broadcasting_fix()
            print("Model loaded successfully")
    
    def process_batch(self, mel_batch: mx.array, batch_idx: int = 0) -> List[Dict]:
        """Process batch with true single-stage word extraction."""
        global _cross_attention_data
        batch_size = mel_batch.shape[0]
        
        # Clear storage for this batch
        _cross_attention_data.clear()
        
        # Replace BatchInference temporarily
        original_batch_inference = mlx_whisper_batch_decoder.BatchInference
        mlx_whisper_batch_decoder.BatchInference = CrossAttentionBatchInference
        
        try:
            # Create batch task with timestamps enabled
            options = DecodingOptions(
                language="en",
                temperature=0.0,
                without_timestamps=False,  # Important for word alignment
                suppress_blank=False
            )
            
            task = BatchDecodingTask(self.model, options)
            
            # Run batch decoding
            print(f"  Decoding {batch_size} chunks...", end='', flush=True)
            start_time = time.time()
            results = task.run(mel_batch)
            decode_time = time.time() - start_time
            print(f" {decode_time:.1f}s")
            
        finally:
            # Restore original
            mlx_whisper_batch_decoder.BatchInference = original_batch_inference
        
        # Process results with word timestamps
        print(f"  Extracting word timestamps...", end='', flush=True)
        word_start = time.time()
        final_results = []
        total_words = 0
        
        for i, result in enumerate(results):
            # Get cross-attention weights for this sequence
            cross_attention_weights = _cross_attention_data.get(i, [])
            
            # Extract words using DTW
            if cross_attention_weights and result.tokens:
                words = extract_words_with_dtw(
                    result.tokens,
                    cross_attention_weights,
                    mel_batch[i].shape,
                    self.model,
                    self.tokenizer,
                    debug=self.debug and i == 0  # Only debug first sequence
                )
                total_words += len(words)
            else:
                words = []
                if self.debug:
                    print(f"\n  Warning: No cross-attention weights for sequence {i}")
            
            final_results.append({
                "text": result.text,
                "tokens": result.tokens,
                "words": words,
                "language": result.language
            })
        
        word_time = time.time() - word_start
        total_time = decode_time + word_time
        throughput = batch_size * 30.0 / total_time
        
        print(f" {word_time:.1f}s")
        print(f"  Total: {total_time:.1f}s ({throughput:.1f}x realtime, {total_words} words)")
        
        return final_results
    
    def process_file(
        self, 
        audio_path: str, 
        batch_size: int = 8,
        save_output: bool = True,
        debug: bool = False
    ) -> Dict:
        """
        Process audio file with optimized batch processing.
        
        Args:
            audio_path: Path to audio file
            batch_size: Number of chunks to process in parallel
            save_output: Whether to save results to JSON
            debug: Enable debug output
            
        Returns:
            Dictionary with transcription results and timing information
        """
        import librosa
        
        self.debug = debug
        self.load()
        
        print(f"\nOptimized WhisperX-MLX Processing")
        print(f"Audio: {audio_path}")
        print(f"Batch size: {batch_size}")
        print("=" * 80)
        
        # Load audio
        print("Loading audio...", end='', flush=True)
        audio, sr = librosa.load(audio_path, sr=16000, mono=True)
        duration = len(audio) / sr
        print(f" Done ({duration/60:.1f} minutes)")
        
        # Create 30-second chunks with proper padding
        chunk_duration = 30.0
        chunk_samples = int(sr * chunk_duration)
        chunks = []
        
        for start in range(0, len(audio), chunk_samples):
            end = min(start + chunk_samples, len(audio))
            chunk = audio[start:end]
            if len(chunk) < chunk_samples:
                chunk = np.pad(chunk, (0, chunk_samples - len(chunk)))
            chunks.append(chunk.astype(np.float32))
        
        print(f"Created {len(chunks)} chunks")
        
        # Process batches
        all_text = []
        all_words = []
        num_batches = (len(chunks) + batch_size - 1) // batch_size
        
        print(f"\nProcessing {num_batches} batches...")
        start_time = time.time()
        
        for batch_idx in range(num_batches):
            start_idx = batch_idx * batch_size
            end_idx = min((batch_idx + 1) * batch_size, len(chunks))
            batch_chunks = chunks[start_idx:end_idx]
            
            print(f"\nBatch {batch_idx + 1}/{num_batches} ({len(batch_chunks)} chunks)")
            
            # Prepare mel spectrograms
            mels = []
            for chunk in batch_chunks:
                mel = log_mel_spectrogram(chunk, n_mels=self.model.dims.n_mels)
                mel = pad_or_trim(mel, 3000, axis=0)  # 3000 frames = 30 seconds
                mels.append(mel)
            
            mel_batch = mx.array(np.stack(mels))
            
            # Process batch
            results = self.process_batch(mel_batch, batch_idx)
            
            # Collect results
            for i, result in enumerate(results):
                all_text.append(result["text"])
                
                # Adjust timestamps for chunk position in full audio
                chunk_offset = (start_idx + i) * chunk_duration
                for word in result["words"]:
                    word_copy = word.copy()
                    word_copy["start"] += chunk_offset
                    word_copy["end"] += chunk_offset
                    all_words.append(word_copy)
        
        total_time = time.time() - start_time
        throughput = duration / total_time
        
        print(f"\n{'='*80}")
        print("PROCESSING COMPLETE")
        print(f"{'='*80}")
        print(f"Total time: {total_time:.1f}s")
        print(f"Throughput: {throughput:.1f}x realtime")
        print(f"Total words: {len(all_words)}")
        
        # Prepare output
        output = {
            "method": "optimized_whisperx_mlx",
            "model": self.model_name,
            "audio_file": audio_path,
            "duration_seconds": duration,
            "processing_time": total_time,
            "throughput": throughput,
            "batch_size": batch_size,
            "text": " ".join(all_text),
            "word_count": len(all_words),
            "words": all_words
        }
        
        # Save results if requested
        if save_output:
            output_file = audio_path.replace('.wav', '_optimized.json')
            output_file = output_file.replace('.mp3', '_optimized.json')
            output_file = output_file.replace('.m4a', '_optimized.json')
            
            with open(output_file, "w") as f:
                json.dump(output, f, indent=2)
            
            print(f"\nResults saved to: {output_file}")
        
        # Show sample words
        if all_words:
            print(f"\nFirst 10 words with timestamps:")
            for i, w in enumerate(all_words[:10]):
                print(f"  {i+1:2d}. {w['word']:15s} [{w['start']:6.2f}s - {w['end']:6.2f}s]")
            
            if len(all_words) > 20:
                print(f"\nLast 10 words with timestamps:")
                for i, w in enumerate(all_words[-10:], len(all_words)-9):
                    print(f"  {i:2d}. {w['word']:15s} [{w['start']:6.2f}s - {w['end']:6.2f}s]")
        
        return output


def main():
    """Main entry point for command-line usage."""
    import sys
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Optimized WhisperX-MLX with accurate word timestamps"
    )
    parser.add_argument("audio_file", help="Path to audio file")
    parser.add_argument(
        "--batch-size", "-b", type=int, default=8,
        help="Batch size for processing (default: 8)"
    )
    parser.add_argument(
        "--model", "-m", default="mlx-community/whisper-large-v3-mlx",
        help="Model to use (default: whisper-large-v3-mlx)"
    )
    parser.add_argument(
        "--no-save", action="store_true",
        help="Don't save output to JSON file"
    )
    parser.add_argument(
        "--debug", "-d", action="store_true",
        help="Enable debug output"
    )
    
    args = parser.parse_args()
    
    # Process audio
    processor = OptimizedWhisperProcessor(model_name=args.model)
    result = processor.process_file(
        args.audio_file,
        batch_size=args.batch_size,
        save_output=not args.no_save,
        debug=args.debug
    )
    
    print(f"\nâœ“ Processing complete!")
    print(f"  - {result['throughput']:.1f}x realtime")
    print(f"  - {result['word_count']} words with accurate timestamps")
    print(f"  - True single-stage processing achieved")


if __name__ == "__main__":
    main()